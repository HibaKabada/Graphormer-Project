# Graphormer-Project

Une implÃ©mentation PyTorch de l'architecture Graphormer pour l'apprentissage sur des graphes, spÃ©cialisÃ©e pour la prÃ©diction de propriÃ©tÃ©s molÃ©culaires.

![image](https://github.com/user-attachments/assets/5dab4e54-bce1-41b8-86bc-d0d362b85bcd)

## ğŸ‘‹ Table des matiÃ¨res

- [Introduction](#introduction)
- [Architecture](#architecture)
- [Installation](#installation)
- [Utilisation](#utilisation)
- [Structure du projet](#structure-du-projet)
- [RÃ©sultats](#rÃ©sultats)
- [RÃ©fÃ©rences](#rÃ©fÃ©rences)
- [Licence](#licence)

## ğŸ” Introduction

Graphormer est une architecture de transformeur adaptÃ©e aux donnÃ©es de graphes, introduisant plusieurs encodages spÃ©cifiques pour capturer efficacement la structure des graphes. Cette implÃ©mentation est spÃ©cialisÃ©e pour la prÃ©diction de propriÃ©tÃ©s molÃ©culaires en utilisant le dataset ESOL (prÃ©diction de solubilitÃ© aqueuse).
- Elle est basÃ©e sur le papier ["Do Transformers Really Perform Bad for Graph Representation?"](https://ar5iv.labs.arxiv.org/html/2106.05234).

### CaractÃ©ristiques principales

- **Encodage de centralitÃ©** - IntÃ©gration de l'importance des nÅ“uds basÃ©e sur les degrÃ©s entrants et sortants
- **Encodage spatial** - Capture de la distance structurelle entre les nÅ“uds
- **Encodage des arÃªtes** - IntÃ©gration de l'information des arÃªtes dans l'attention
- **Multi-head attention** - Attention Ã  plusieurs tÃªtes adaptÃ©e aux graphes

## ğŸ° Architecture

L'architecture Graphormer Ã©tend le modÃ¨le Transformeur aux donnÃ©es de graphes en intÃ©grant la structure du graphe dans le mÃ©canisme d'attention.

```mermaid
graph TD
    A[DonnÃ©es d'entrÃ©e - Graphe] --> B[Encodage de centralitÃ©]
    A --> C[Encodage spatial]
    A --> D[Encodage des arÃªtes]
    B --> E[Layer Graphormer]
    C --> E
    D --> E
    E --> F[Layer Graphormer suivant...]
    F --> G[Global Mean Pooling]
    G --> H[Output Layer]
    H --> I[PrÃ©diction]
```

### Composants clÃ©s

1. **CentralityEncoding** - Capture l'importance des nÅ“uds basÃ©e sur leur degrÃ©
2. **SpatialEncoding** - Encode la distance structurelle entre les nÅ“uds
3. **EdgeEncoding** - IntÃ¨gre l'information des arÃªtes
4. **GraphormerAttentionHead** - MÃ©canisme d'attention adaptÃ© aux graphes
5. **GraphormerMultiHeadAttention** - Attention Ã  plusieurs tÃªtes
6. **GraphormerEncoderLayer** - Couche complÃ¨te du Graphormer

## ğŸ’» Installation

### PrÃ©requis

- Python 3.8+
- PyTorch 1.9+
- PyTorch Geometric
- NetworkX
- tqdm
- texttable

### Installation des dÃ©pendances

```bash
# CrÃ©er un environnement virtuel
python -m venv venv
source venv/bin/activate  # Sur Windows: venv\Scripts\activate

# Installer PyTorch (vÃ©rifier la compatibilitÃ© avec votre CUDA)
pip install torch

# Installer PyTorch Geometric
pip install torch-geometric

# Autres dÃ©pendances
pip install networkx tqdm texttable scikit-learn
```

## ğŸš€ Utilisation

### EntraÃ®nement du modÃ¨le

```bash
python main.py --exp_name "Exp1" --epochs 100 --lr 0.0005 --num_layers 1 --node_dim 128 --edge_dim 128 --num_heads 4
```

### Options principales

| ParamÃ¨tre | Description | Valeur par dÃ©faut |
|-----------|-------------|-------------------|
| `--exp_name` | Nom de l'expÃ©rience | Exp |
| `--train_batch_size` | Taille du batch d'entraÃ®nement | 64 |
| `--test_batch_size` | Taille du batch de test | 64 |
| `--gpu_index` | Index GPU (nÃ©gatif pour CPU) | 0 |
| `--epochs` | Nombre d'Ã©poques | 100 |
| `--lr` | Taux d'apprentissage | 0.0005 |
| `--num_layers` | Nombre de couches Graphormer | 1 |
| `--node_dim` | Dimension des features des nÅ“uds | 128 |
| `--edge_dim` | Dimension des features des arÃªtes | 128 |
| `--num_heads` | Nombre de tÃªtes d'attention | 4 |
| `--max_path_distance` | Distance maximale entre deux nÅ“uds | 5 |

## ğŸ“ Structure du projet

```
graphormer/
â”‚
â”œâ”€â”€ data.py           # Chargement et prÃ©traitement des donnÃ©es
â”œâ”€â”€ layer.py          # DÃ©finition des couches Graphormer
â”œâ”€â”€ main.py           # Script principal d'entraÃ®nement et d'Ã©valuation
â”œâ”€â”€ model.py          # DÃ©finition du modÃ¨le Graphormer complet
â”œâ”€â”€ parameter.py      # Gestion des paramÃ¨tres et configuration

```

## ğŸ“Š RÃ©sultats

### Performance sur le dataset ESOL

| MÃ©trique | Valeur |
|----------|--------|
| MAE (test) | 0.964814 |

#### Temps d'entraÃ®nement

L'entraÃ®nement complet a pris environ 7 heures sur un GPU standard, avec les temps moyens suivants par batch:
- DÃ©but d'entraÃ®nement: ~21-27 it/s
- Milieu d'entraÃ®nement: ~28-30 it/s
- Fin d'entraÃ®nement: ~27-30 it/s

La mÃ©trique MAE (Mean Absolute Error) finale sur l'ensemble de test indique que notre modÃ¨le prÃ©dit les valeurs de solubilitÃ© avec une prÃ©cision raisonnable pour le dataset ESOL.

### Progression de l'entraÃ®nement

Le modÃ¨le a Ã©tÃ© entraÃ®nÃ© pendant 100 Ã©poques. Voici un extrait des logs d'entraÃ®nement montrant la progression de l'erreur:

![image](https://github.com/user-attachments/assets/d296177d-7167-470d-b9a0-25d7d1dae388)



## Note sur le dataset utilisÃ©

âš ï¸ **Important :** L'article original "Do Transformers Really Perform Bad for Graph Representation?" Ã©value l'architecture Graphormer sur plusieurs datasets, notamment PCQM4M, MolPCBA et d'autres benchmarks importants. Notre implÃ©mentation se concentre uniquement sur le dataset ESOL (solubilitÃ© aqueuse) en raison de contraintes de ressources computationnelles.

Ces datasets de l'article original sont considÃ©rablement plus volumineux et nÃ©cessitent des ressources GPU importantes pour l'entraÃ®nement, ce qui n'Ã©tait pas disponible dans notre environnement de dÃ©veloppement. Le dataset ESOL a Ã©tÃ© choisi comme alternative viable car:

1. Sa taille est plus adaptÃ©e aux ressources disponibles
2. Il permet tout de mÃªme de dÃ©montrer l'efficacitÃ© de l'architecture Graphormer

Cette adaptation nous a permis d'implÃ©menter et de tester les concepts clÃ©s de l'architecture Graphormer (encodage de centralitÃ©, encodage spatial, encodage des arÃªtes) dans un contexte pratique, tout en obtenant des rÃ©sultats significatifs sur la tÃ¢che de prÃ©diction de solubilitÃ©.

## ğŸ“š RÃ©fÃ©rences

- [Paper original Graphormer](https://ar5iv.labs.arxiv.org/html/2106.05234)
- [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/)
- [MoleculeNet](https://moleculenet.org/)
  

## ğŸ“„ Licence

Ce projet est sous licence MIT - voir le fichier LICENSE pour plus de dÃ©tails.
